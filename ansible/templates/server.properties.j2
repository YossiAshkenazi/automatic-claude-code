# Kafka Server Configuration Template
# High-Performance Configuration for 100K+ msgs/sec throughput
# Generated by Ansible for broker {{ kafka_config['broker.id'] }}

############################# Server Basics #############################
# The id of the broker. This must be set to a unique integer for each broker.
broker.id={{ kafka_config['broker.id'] }}

############################# Socket Server Settings #############################
# The address the socket server listens on.
listeners={{ kafka_config.listeners }}

# Hostname and port the broker will advertise to producers and consumers.
advertised.listeners={{ kafka_config['advertised.listeners'] }}

# Maps listener names to security protocols
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network
num.network.threads={{ kafka_config['num.network.threads'] }}

# The number of threads that the server uses for processing requests
num.io.threads={{ kafka_config['num.io.threads'] }}

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes={{ kafka_config['socket.send.buffer.bytes'] }}

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes={{ kafka_config['socket.receive.buffer.bytes'] }}

# The maximum size of a request that the socket server will accept
socket.request.max.bytes={{ kafka_config['socket.request.max.bytes'] }}

############################# Log Basics #############################
# A comma separated list of directories under which to store log files
log.dirs={{ kafka_config['log.dirs'] }}

# The default number of log partitions per topic
num.partitions=12

# The number of threads per data directory to be used for log recovery at startup
num.recovery.threads.per.data.dir={{ kafka_config['num.recovery.threads.per.data.dir'] }}

############################# Internal Topic Settings #############################
# The replication factor for the group metadata internal topics
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2

############################# Log Flush Policy #############################
# The number of messages to accept before forcing a flush of data to disk
log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
log.flush.interval.ms=1000

############################# Log Retention Policy #############################
# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours={{ kafka_config['log.retention.hours'] }}

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes={{ kafka_config['log.segment.bytes'] }}

# The interval at which log segments are checked to see if they can be deleted
log.retention.check.interval.ms=300000

# The maximum size of the log before deleting it
log.retention.bytes={{ kafka_config['log.retention.bytes'] }}

# Log cleanup policy
log.cleanup.policy={{ kafka_config['log.cleanup.policy'] }}

# The amount of time to retain delete tombstone markers for log compacted topics
log.cleaner.delete.retention.ms=86400000

############################# Replication Settings #############################
# The default replication factor for automatically created topics
default.replication.factor={{ kafka_config['default.replication.factor'] }}

# The minimum number of replicas that must acknowledge a write for the write to be considered successful
min.insync.replicas={{ kafka_config['min.insync.replicas'] }}

# Whether to enable replicas not in the ISR set to be elected as leader as a last resort
unclean.leader.election.enable={{ kafka_config['unclean.leader.election.enable'] | lower }}

# Number of fetcher threads used to replicate messages from a source broker
num.replica.fetchers={{ kafka_config['num.replica.fetchers'] }}

# The frequency with which the replica high water mark is updated
replica.lag.time.max.ms=30000

############################# Zookeeper/KRaft Configuration #############################
{% if not (use_kraft_mode | bool) %}
# Zookeeper connection string
zookeeper.connect={{ kafka_config['zookeeper.connect'] }}

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000

# The max time that the client waits to establish a connection to zookeeper
zookeeper.session.timeout.ms=18000
{% else %}
# KRaft mode configuration
process.roles=broker,controller
node.id={{ kafka_config['broker.id'] }}
controller.quorum.voters=1@{{ groups['kafka_brokers'][0] }}:9093,2@{{ groups['kafka_brokers'][1] }}:9093,3@{{ groups['kafka_brokers'][2] }}:9093
listeners=PLAINTEXT://:9092,CONTROLLER://:9093
inter.broker.listener.name=PLAINTEXT
controller.listener.names=CONTROLLER
log.dirs={{ kafka_config['log.dirs'] }}
{% endif %}

############################# Producer/Consumer Optimization #############################
# Compression type for all data generated by the broker
compression.type={{ kafka_config['compression.type'] }}

# The maximum amount of time the client will wait for the response of a request
request.timeout.ms=30000

# The amount of time to wait before attempting to retry a failed request
retry.backoff.ms=100

# Enable idempotent producer
enable.idempotence=true

############################# Performance Tuning #############################
# The maximum number of connections we allow from each ip address
max.connections.per.ip=1000

# The maximum number of connections we allow in total
max.connections=2000

# The number of background threads to use for various background processing tasks
background.threads=10

# The amount of time to wait before attempting to reconnect to a given host
reconnect.backoff.ms=50

# The configuration controls the maximum amount of time the client will wait for the response of a request
request.timeout.ms=30000

# The frequency with which we save out the partition assignments to zookeeper
auto.leader.rebalance.enable=true

# The ratio of leader imbalance allowed per broker
leader.imbalance.per.broker.percentage=10

# The frequency with which the partition rebalance check is triggered by the controller
leader.imbalance.check.interval.seconds=300

# Rack awareness for replica placement
{% if ansible_availability_zone is defined %}
broker.rack={{ ansible_availability_zone }}
{% endif %}

############################# SSL Configuration #############################
{% if enable_ssl | default(true) %}
# SSL keystore location and password
ssl.keystore.location=/opt/kafka/ssl/{{ inventory_hostname }}.keystore.jks
ssl.keystore.password={{ ssl_keystore_password | default('changeme') }}
ssl.key.password={{ ssl_keystore_password | default('changeme') }}

# SSL truststore location and password
ssl.truststore.location=/opt/kafka/ssl/kafka.truststore.jks
ssl.truststore.password={{ ssl_truststore_password | default('changeme') }}

# SSL client authentication
ssl.client.auth=none

# SSL enabled protocols
ssl.enabled.protocols=TLSv1.2,TLSv1.3

# SSL cipher suites
ssl.cipher.suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256

# Inter-broker security protocol
security.inter.broker.protocol={% if enable_sasl | default(true) %}SASL_SSL{% else %}SSL{% endif %}
{% endif %}

############################# SASL Configuration #############################
{% if enable_sasl | default(true) %}
# SASL mechanism for inter-broker communication
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN,SCRAM-SHA-256,SCRAM-SHA-512

# SASL configuration
listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required;
listener.name.sasl_ssl.scram-sha-512.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required;
{% endif %}

############################# JMX Monitoring #############################
{% if kafka_config['jmx.port'] is defined %}
# JMX port for monitoring
jmx.port={{ kafka_config['jmx.port'] }}
{% endif %}

############################# Group Coordinator Settings #############################
# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay
group.initial.rebalance.delay.ms=3000

# The maximum allowed session timeout for registered consumers
group.max.session.timeout.ms=1800000

# The minimum allowed session timeout for registered consumers
group.min.session.timeout.ms=6000

############################# Metrics Configuration #############################
# Enable built-in metrics reporters
metric.reporters=org.apache.kafka.common.metrics.JmxReporter

# The frequency at which the metrics are updated
metrics.sample.window.ms=30000

# The number of samples to use when computing metrics
metrics.num.samples=2

# Auto create topics
auto.create.topics.enable=false

# Enable topic deletion
delete.topic.enable=true

############################# Advanced Performance Settings #############################
# Increase batch size for better throughput
batch.size=16384

# Increase linger time for better throughput (vs latency tradeoff)
linger.ms=5

# Buffer memory for producer
buffer.memory=33554432

# Fetch minimum size for consumers
fetch.min.bytes=1

# Maximum wait time for fetch requests
fetch.max.wait.ms=500

# Maximum bytes per fetch request
fetch.max.bytes=52428800

# Maximum bytes per partition in fetch
max.partition.fetch.bytes=1048576

# Connection idle timeout
connections.max.idle.ms=540000

# Replica socket timeout
replica.socket.timeout.ms=30000

# Replica socket receive buffer
replica.socket.receive.buffer.bytes=65536

# Controller socket timeout
controller.socket.timeout.ms=30000

# Maximum time to wait for leader election
leader.election.timeout.ms=30000

# Log index size
log.index.size.max.bytes=10485760

# Log index interval
log.index.interval.bytes=4096

# Log segment delete delay
log.segment.delete.delay.ms=60000

# Log flush scheduler interval
log.flush.scheduler.interval.ms=9223372036854775807

# Offset commit timeout
offsets.commit.timeout.ms=5000

# Offset commit required acks
offsets.commit.required.acks=-1